<!DOCTYPE html>
<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="description" content="Claude Shannon">
    <title>Info on Claude Shannon</title>
  </head>
  <body>
    Claude E. Shannon.
    The scientist who single-handedly launched classical information theory. -scientific American

    Classical information science sprang forth about 50 years ago, with the primary work of one man. In a paper written at Bell Labs in 1948, Shannon nnndefined in mathematical terms what information is and how it can be transmitted in the face of noice. What had been viewed as quite distinct modes of communication -- the telegraph, telephone, radio and television -- were unified in a single framework.

    Shannon was born in 1916 in Petoskey, Michigan, the son of a judge and a teacher. Among other inventive endeavors, as a youth he build a telegraph from his house to a friend's out of fencing wire. He graduated from the University of Michigan with degrees in electrical engineering and mathematics in 1936 and went to M.I.T., where he worked under computer pioneer Vannevar Bush on an analog computer called the differential analyzer.
    Shannon's M.I.T. master's thesis in electrical engineering has been called the most important of the 20th century: in it the 22-year-old Shannon showed how the logical algebra of 19th-century mathematician George Boole could be implemented using electronic circuits of relays and switches. This most fundamental feature of digital computers' design--the representation of "true" and "false" and "0" and "1" as open or closed switches, and the use of electronic logic gates to make decisions and to carry out arithmetic--can be traced back to the insights in Shannon's thesis.
    In 1941, with a Ph.D. in mathematics under his belt, Shannon went to Bell Labs, where he worked on war-related matters, including cryptography. Unknown to those around him, he was also working on the theory behind information and communications. In 1948 this work emerged in a celebrated paper published in two parts in Bell Labs's research journal.

    Shannon defined the quantity of information produced by a source -- for example, the quantity in a message -- by a formular similar to the equation that defines thermodynamic entropy in physics. In the dawn of the information age, this digitizing of information of any sort was a revolutionary step. His paper may have been the first to use the word "bit," short for binary digit.
  </body>
</html>
